import nltk
from nltk.util import ngrams
from collections import defaultdict, Counter
import numpy as np
import tensorflow as tf
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense

# Sample text data
text = "I am learning NLP. I am enjoying it. I will use NLP for many projects."

# Preprocess text
tokens = nltk.word_tokenize(text)

# Generate bigrams
bigrams = list(ngrams(tokens, 2))

# Create a frequency distribution of bigrams
bigram_freq = defaultdict(Counter)
for w1, w2 in bigrams:
    bigram_freq[w1][w2] += 1

# Function to predict the next word using N-gram model
def predict_next_word_ngram(word, n=1):
    if word in bigram_freq:
        return bigram_freq[word].most_common(n)
    else:
        return [(None, 0)]

# Example usage of N-gram model
print("N-gram model prediction for 'I':", predict_next_word_ngram('I'))

# Tokenize the text for RNN
tokenizer = Tokenizer()
tokenizer.fit_on_texts([text])
total_words = len(tokenizer.word_index) + 1

# Create input sequences using the tokens
input_sequences = []
for i in range(1, len(tokenizer.word_index)):
    n_gram_sequence = tokenizer.texts_to_sequences([text])[0][:i+1]
    input_sequences.append(n_gram_sequence)

# Pad sequences to ensure uniform input length
max_sequence_len = max([len(seq) for seq in input_sequences])
input_sequences = np.array(pad_sequences(input_sequences, maxlen=max_sequence_len, padding='pre'))

# Create predictors and label
xs, labels = input_sequences[:, :-1], input_sequences[:, -1]

# One-hot encode the labels
ys = tf.keras.utils.to_categorical(labels, num_classes=total_words)

# Define the RNN model
model = Sequential()
model.add(Embedding(total_words, 10, input_length=max_sequence_len-1))
model.add(LSTM(100))
model.add(Dense(total_words, activation='softmax'))

# Compile the model
model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])

# Train the model
model.fit(xs, ys, epochs=100, verbose=1)

# Function to predict the next word using RNN model
def predict_next_word_lstm(model, tokenizer, text, max_sequence_len):
    token_list = tokenizer.texts_to_sequences([text])[0]
    token_list = pad_sequences([token_list], maxlen=max_sequence_len-1, padding='pre')
    predicted = model.predict(token_list, verbose=0)
    predicted_word_index = np.argmax(predicted, axis=1)
    return tokenizer.index_word[predicted_word_index[0]]

# Example usage of RNN model
print("RNN model prediction for 'I am':", predict_next_word_lstm(model, tokenizer, "I am", max_sequence_len))